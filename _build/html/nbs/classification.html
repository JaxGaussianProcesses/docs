
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Classification &#8212; My sample book</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/css/gpjax_theme.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/sphinx_highlight.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Sparse Gaussian Process Regression" href="collapsed_vi.html" />
    <link rel="prev" title="Regression" href="regression.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      <img src="../_static/../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">My sample book</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Welcome to GPJax!
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro_to_gps.html">
   New to Gaussian Processes?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="regression.html">
   Regression
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="collapsed_vi.html">
   Sparse Gaussian Process Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="uncollapsed_vi.html">
   Sparse Stochastic Variational Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="barycentres.html">
   Gaussian Processes Barycentres
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="haiku.html">
   Deep Kernel Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kernels.html">
   Kernel Guide
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="graph_kernels.html">
   Graph Kernels
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="tfp_integration.html">
   TensorFlow Probability Integration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="natgrads.html">
   Natural Gradients
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="yacht.html">
   UCI Data Benchmarking
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/docs/nbs/classification.py"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/executablebooks/jupyter-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fnbs/classification.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/nbs/classification.pct.py"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.pct.py</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dataset">
   Dataset
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#map-inference">
   MAP inference
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#laplace-approximation">
   Laplace approximation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mcmc-inference">
   MCMC inference
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mcmc-through-blackjax">
     MCMC through BlackJax
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sampler-efficiency">
     Sampler efficiency
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#prediction">
   Prediction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#system-configuration">
   System configuration
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Classification</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dataset">
   Dataset
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#map-inference">
   MAP inference
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#laplace-approximation">
   Laplace approximation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#mcmc-inference">
   MCMC inference
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mcmc-through-blackjax">
     MCMC through BlackJax
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#sampler-efficiency">
     Sampler efficiency
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#prediction">
   Prediction
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#system-configuration">
   System configuration
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="classification">
<h1>Classification<a class="headerlink" href="#classification" title="Permalink to this heading">#</a></h1>
<p>In this notebook we demonstrate how to perform inference for Gaussian process models with non-Gaussian likelihoods via maximum a posteriori (MAP) and Markov chain Monte Carlo (MCMC). We focus on a classification task here and use <a class="reference external" href="https://github.com/blackjax-devs/blackjax/">BlackJax</a> for sampling.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">blackjax</span>
<span class="kn">import</span> <span class="nn">distrax</span> <span class="k">as</span> <span class="nn">dx</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="kn">import</span> <span class="nn">jax.random</span> <span class="k">as</span> <span class="nn">jr</span>
<span class="kn">import</span> <span class="nn">jax.scipy</span> <span class="k">as</span> <span class="nn">jsp</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">optax</span> <span class="k">as</span> <span class="nn">ox</span>
<span class="kn">from</span> <span class="nn">jax.config</span> <span class="kn">import</span> <span class="n">config</span>
<span class="kn">from</span> <span class="nn">jaxtyping</span> <span class="kn">import</span> <span class="n">Array</span><span class="p">,</span> <span class="n">Float</span>
<span class="kn">from</span> <span class="nn">jaxutils</span> <span class="kn">import</span> <span class="n">Dataset</span>
<span class="kn">import</span> <span class="nn">jaxkern</span> <span class="k">as</span> <span class="nn">jk</span>
<span class="kn">import</span> <span class="nn">jax</span>

<span class="kn">import</span> <span class="nn">gpjax</span> <span class="k">as</span> <span class="nn">gpx</span>

<span class="c1"># Enable Float64 for more stable matrix inversions.</span>
<span class="n">config</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="s2">&quot;jax_enable_x64&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">I</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">eye</span>
<span class="n">key</span> <span class="o">=</span> <span class="n">jr</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="dataset">
<h2>Dataset<a class="headerlink" href="#dataset" title="Permalink to this heading">#</a></h2>
<p>With the necessary modules imported, we simulate a dataset <span class="math notranslate nohighlight">\(\mathcal{D} = (, \boldsymbol{y}) = \{(x_i, y_i)\}_{i=1}^{100}\)</span> with inputs <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> sampled uniformly on <span class="math notranslate nohighlight">\((-1., 1)\)</span> and corresponding binary outputs</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{y} = 0.5 * \text{sign}(\cos(2 *  + \boldsymbol{\epsilon})) + 0.5, \quad \boldsymbol{\epsilon} \sim \mathcal{N} \left(\textbf{0}, \textbf{I} * (0.05)^{2} \right).\]</div>
<p>We store our data <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> as a GPJax <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> and create test inputs for later.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">jr</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">minval</span><span class="o">=-</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="mf">1.0</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="mi">3</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="n">jr</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.05</span><span class="p">))</span> <span class="o">+</span> <span class="mf">0.5</span>

<span class="n">D</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

<span class="n">xtest</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>[&lt;matplotlib.lines.Line2D at 0x282818ca0&gt;]
</pre></div>
</div>
<img alt="../_images/classification_3_1.png" src="../_images/classification_3_1.png" />
</div>
</div>
</section>
<section id="map-inference">
<h2>MAP inference<a class="headerlink" href="#map-inference" title="Permalink to this heading">#</a></h2>
<p>We begin by defining a Gaussian process prior with a radial basis function (RBF) kernel, chosen for the purpose of exposition. Since our observations are binary, we choose a Bernoulli likelihood with a probit link function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">kernel</span> <span class="o">=</span> <span class="n">jk</span><span class="o">.</span><span class="n">RBF</span><span class="p">()</span>
<span class="n">prior</span> <span class="o">=</span> <span class="n">gpx</span><span class="o">.</span><span class="n">Prior</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="n">kernel</span><span class="p">)</span>
<span class="n">likelihood</span> <span class="o">=</span> <span class="n">gpx</span><span class="o">.</span><span class="n">Bernoulli</span><span class="p">(</span><span class="n">num_datapoints</span><span class="o">=</span><span class="n">D</span><span class="o">.</span><span class="n">n</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We construct the posterior through the product of our prior and likelihood.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">posterior</span> <span class="o">=</span> <span class="n">prior</span> <span class="o">*</span> <span class="n">likelihood</span>
<span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">posterior</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;class &#39;gpjax.gps.NonConjugatePosterior&#39;&gt;
</pre></div>
</div>
</div>
</div>
<p>Whilst the latent function is Gaussian, the posterior distribution is non-Gaussian since our generative model first samples the latent GP and propagates these samples through the likelihood function’s inverse link function. This step prevents us from being able to analytically integrate the latent function’s values out of our posterior, and we must instead adopt alternative inference techniques. We begin with maximum a posteriori (MAP) estimation, a fast inference procedure to obtain point estimates for the latent function and the kernel’s hyperparameters by maximising the marginal log-likelihood.</p>
<p>To begin we obtain an initial parameter state through the <code class="docutils literal notranslate"><span class="pre">initialise</span></code> callable (see the <a class="reference external" href="https://gpjax.readthedocs.io/en/latest/nbs/regression.html">regression notebook</a>). We can obtain a MAP estimate by optimising the marginal log-likelihood with Optax’s optimisers.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameter_state</span> <span class="o">=</span> <span class="n">gpx</span><span class="o">.</span><span class="n">initialise</span><span class="p">(</span><span class="n">posterior</span><span class="p">)</span>
<span class="n">negative_mll</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="n">posterior</span><span class="o">.</span><span class="n">marginal_log_likelihood</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">negative</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>

<span class="n">optimiser</span> <span class="o">=</span> <span class="n">ox</span><span class="o">.</span><span class="n">adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="n">inference_state</span> <span class="o">=</span> <span class="n">gpx</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">objective</span><span class="o">=</span><span class="n">negative_mll</span><span class="p">,</span>
    <span class="n">parameter_state</span><span class="o">=</span><span class="n">parameter_state</span><span class="p">,</span>
    <span class="n">optax_optim</span><span class="o">=</span><span class="n">optimiser</span><span class="p">,</span>
    <span class="n">num_iters</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">map_estimate</span><span class="p">,</span> <span class="n">training_history</span> <span class="o">=</span> <span class="n">inference_state</span><span class="o">.</span><span class="n">unpack</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/var/folders/qb/c3b8rx055411rkfvdyzty9_80000gn/T/ipykernel_3983/4266840888.py:1: UserWarning: No PRNGKey specified. Defaulting to seed 123.
  parameter_state = gpx.initialise(posterior)
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "7aaeec6ff8274fa5aa85e307d5004579"}
</script></div>
</div>
<p>From which we can make predictions at novel inputs, as illustrated below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">map_latent_dist</span> <span class="o">=</span> <span class="n">posterior</span><span class="p">(</span><span class="n">map_estimate</span><span class="p">,</span> <span class="n">D</span><span class="p">)(</span><span class="n">xtest</span><span class="p">)</span>

<span class="n">predictive_dist</span> <span class="o">=</span> <span class="n">likelihood</span><span class="p">(</span><span class="n">map_estimate</span><span class="p">,</span> <span class="n">map_latent_dist</span><span class="p">)</span>

<span class="n">predictive_mean</span> <span class="o">=</span> <span class="n">predictive_dist</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">predictive_std</span> <span class="o">=</span> <span class="n">predictive_dist</span><span class="o">.</span><span class="n">stddev</span><span class="p">()</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Observations&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:red&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xtest</span><span class="p">,</span> <span class="n">predictive_mean</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Predictive mean&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
    <span class="n">xtest</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span>
    <span class="n">predictive_mean</span> <span class="o">-</span> <span class="n">predictive_std</span><span class="p">,</span>
    <span class="n">predictive_mean</span> <span class="o">+</span> <span class="n">predictive_std</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Two sigma&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">xtest</span><span class="p">,</span>
    <span class="n">predictive_mean</span> <span class="o">-</span> <span class="n">predictive_std</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span>
    <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span>
    <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">xtest</span><span class="p">,</span>
    <span class="n">predictive_mean</span> <span class="o">+</span> <span class="n">predictive_std</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span>
    <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span>
    <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x173ce25b0&gt;
</pre></div>
</div>
<img alt="../_images/classification_12_1.png" src="../_images/classification_12_1.png" />
</div>
</div>
<p>Here we projected the map estimates <span class="math notranslate nohighlight">\(\hat{\boldsymbol{f}}\)</span> for the function values <span class="math notranslate nohighlight">\(\boldsymbol{f}\)</span> at the data points <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> to get predictions over the whole domain,</p>
<p>\begin{align}
p(f(\cdot)| \mathcal{D})  \approx q_{map}(f(\cdot)) := \int p(f(\cdot)| \boldsymbol{f}) \delta(\boldsymbol{f} - \hat{\boldsymbol{f}}) d \boldsymbol{f} = \mathcal{N}(\mathbf{K}<em>{\boldsymbol{(\cdot)x}}  \mathbf{K}</em>{\boldsymbol{xx}}^{-1} \hat{\boldsymbol{f}},  \mathbf{K}<em>{\boldsymbol{(\cdot, \cdot)}} - \mathbf{K}</em>{\boldsymbol{(\cdot)\boldsymbol{x}}} \mathbf{K}<em>{\boldsymbol{xx}}^{-1} \mathbf{K}</em>{\boldsymbol{\boldsymbol{x}(\cdot)}}).
\end{align}</p>
<p>However, as a point estimate, MAP estimation is severely limited for uncertainty quantification, providing only a single piece of information about the posterior.</p>
</section>
<section id="laplace-approximation">
<h2>Laplace approximation<a class="headerlink" href="#laplace-approximation" title="Permalink to this heading">#</a></h2>
<p>The Laplace approximation improves uncertainty quantification by incorporating curvature induced by the marginal log-likelihood’s Hessian to construct an approximate Gaussian distribution centered on the MAP estimate. Writing <span class="math notranslate nohighlight">\(\tilde{p}(\boldsymbol{f}|\mathcal{D}) = p(\boldsymbol{y}|\boldsymbol{f}) p(\boldsymbol{f})\)</span> as the unormalised posterior for function values <span class="math notranslate nohighlight">\(\boldsymbol{f}\)</span> at the datapoints <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span>, we can expand the log of this about the posterior mode <span class="math notranslate nohighlight">\(\hat{\boldsymbol{f}}\)</span> via a Taylor expansion. This gives:</p>
<p>\begin{align}
\log\tilde{p}(\boldsymbol{f}|\mathcal{D}) = \log\tilde{p}(\hat{\boldsymbol{f}}|\mathcal{D}) + \left[\nabla \log\tilde{p}({\boldsymbol{f}}|\mathcal{D})|<em>{\hat{\boldsymbol{f}}}\right]^{T} (\boldsymbol{f}-\hat{\boldsymbol{f}}) + \frac{1}{2} (\boldsymbol{f}-\hat{\boldsymbol{f}})^{T} \left[\nabla^2 \tilde{p}(\boldsymbol{y}|\boldsymbol{f})|</em>{\hat{\boldsymbol{f}}} \right] (\boldsymbol{f}-\hat{\boldsymbol{f}}) + \mathcal{O}(\lVert \boldsymbol{f} - \hat{\boldsymbol{f}} \rVert^3).
\end{align}</p>
<p>Now since <span class="math notranslate nohighlight">\(\nabla \log\tilde{p}({\boldsymbol{f}}|\mathcal{D})\)</span> is zero at the mode, this suggests the following approximation
\begin{align}
\tilde{p}(\boldsymbol{f}|\mathcal{D}) \approx \log\tilde{p}(\hat{\boldsymbol{f}}|\mathcal{D}) \exp\left{ \frac{1}{2} (\boldsymbol{f}-\hat{\boldsymbol{f}})^{T} \left[-\nabla^2 \tilde{p}(\boldsymbol{y}|\boldsymbol{f})|_{\hat{\boldsymbol{f}}} \right] (\boldsymbol{f}-\hat{\boldsymbol{f}}) \right}
\end{align},</p>
<p>that we identify as a Gaussian distribution,  <span class="math notranslate nohighlight">\(p(\boldsymbol{f}| \mathcal{D}) \approx q(\boldsymbol{f}) := \mathcal{N}(\hat{\boldsymbol{f}}, [-\nabla^2 \tilde{p}(\boldsymbol{y}|\boldsymbol{f})|_{\hat{\boldsymbol{f}}} ]^{-1} )\)</span>. Since the negative Hessian is positive definite, we can use the Cholesky decomposition to obtain the covariance matrix of the Laplace approximation at the datapoints below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gram</span><span class="p">,</span> <span class="n">cross_covariance</span> <span class="o">=</span> <span class="p">(</span><span class="n">kernel</span><span class="o">.</span><span class="n">gram</span><span class="p">,</span> <span class="n">kernel</span><span class="o">.</span><span class="n">cross_covariance</span><span class="p">)</span>
<span class="n">jitter</span> <span class="o">=</span> <span class="mf">1e-6</span>

<span class="c1"># Compute (latent) function value map estimates at training points:</span>
<span class="n">Kxx</span> <span class="o">=</span> <span class="n">gram</span><span class="p">(</span><span class="n">map_estimate</span><span class="p">[</span><span class="s2">&quot;kernel&quot;</span><span class="p">],</span> <span class="n">x</span><span class="p">)</span>
<span class="n">Kxx</span> <span class="o">+=</span> <span class="n">I</span><span class="p">(</span><span class="n">D</span><span class="o">.</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="n">jitter</span>
<span class="n">Lx</span> <span class="o">=</span> <span class="n">Kxx</span><span class="o">.</span><span class="n">to_root</span><span class="p">()</span>
<span class="n">f_hat</span> <span class="o">=</span> <span class="n">Lx</span> <span class="o">@</span> <span class="n">map_estimate</span><span class="p">[</span><span class="s2">&quot;latent&quot;</span><span class="p">]</span>

<span class="c1"># Negative Hessian,  H = -∇²p_tilde(y|f):</span>
<span class="n">H</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jacfwd</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">jacrev</span><span class="p">(</span><span class="n">negative_mll</span><span class="p">))(</span><span class="n">map_estimate</span><span class="p">)[</span><span class="s2">&quot;latent&quot;</span><span class="p">][</span><span class="s2">&quot;latent&quot;</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span>

<span class="c1"># LLᵀ = H</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">H</span> <span class="o">+</span> <span class="n">I</span><span class="p">(</span><span class="n">D</span><span class="o">.</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="n">jitter</span><span class="p">)</span>

<span class="c1"># H⁻¹ = H⁻¹ I = (LLᵀ)⁻¹ I = L⁻ᵀL⁻¹ I</span>
<span class="n">L_inv</span> <span class="o">=</span> <span class="n">jsp</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve_triangular</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">I</span><span class="p">(</span><span class="n">D</span><span class="o">.</span><span class="n">n</span><span class="p">),</span> <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">H_inv</span> <span class="o">=</span> <span class="n">jsp</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">solve_triangular</span><span class="p">(</span><span class="n">L</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">L_inv</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">laplace_approximation</span> <span class="o">=</span> <span class="n">dx</span><span class="o">.</span><span class="n">MultivariateNormalFullCovariance</span><span class="p">(</span><span class="n">f_hat</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span> <span class="n">H_inv</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>For novel inputs, we must project the above approximating distribution through the Gaussian conditional distribution <span class="math notranslate nohighlight">\(p(f(\cdot)| \boldsymbol{f})\)</span>,</p>
<p>\begin{align}
p(f(\cdot)| \mathcal{D}) \approx q_{Laplace}(f(\cdot)) := \int p(f(\cdot)| \boldsymbol{f}) q(\boldsymbol{f}) d \boldsymbol{f} = \mathcal{N}(\mathbf{K}<em>{\boldsymbol{(\cdot)x}}  \mathbf{K}</em>{\boldsymbol{xx}}^{-1} \hat{\boldsymbol{f}},  \mathbf{K}<em>{\boldsymbol{(\cdot, \cdot)}} - \mathbf{K}</em>{\boldsymbol{(\cdot)\boldsymbol{x}}} \mathbf{K}<em>{\boldsymbol{xx}}^{-1} (\mathbf{K}</em>{\boldsymbol{xx}} - [-\nabla^2 \tilde{p}(\boldsymbol{y}|\boldsymbol{f})|<em>{\hat{\boldsymbol{f}}} ]^{-1}) \mathbf{K}</em>{\boldsymbol{xx}}^{-1} \mathbf{K}_{\boldsymbol{\boldsymbol{x}(\cdot)}}).
\end{align}</p>
<p>This is the same approximate distribution <span class="math notranslate nohighlight">\(q_{map}(f(\cdot))\)</span>, but we have pertubed the covariance by a curvature term of <span class="math notranslate nohighlight">\(\mathbf{K}_{\boldsymbol{(\cdot)\boldsymbol{x}}} \mathbf{K}_{\boldsymbol{xx}}^{-1} [-\nabla^2 \tilde{p}(\boldsymbol{y}|\boldsymbol{f})|_{\hat{\boldsymbol{f}}} ]^{-1} \mathbf{K}_{\boldsymbol{xx}}^{-1} \mathbf{K}_{\boldsymbol{\boldsymbol{x}(\cdot)}}\)</span>. We take the latent distribution computed in the previous section and add this term to the covariance to construct <span class="math notranslate nohighlight">\(q_{Laplace}(f(\cdot))\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">construct_laplace</span><span class="p">(</span><span class="n">test_inputs</span><span class="p">:</span> <span class="n">Float</span><span class="p">[</span><span class="n">Array</span><span class="p">,</span> <span class="s2">&quot;N D&quot;</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">dx</span><span class="o">.</span><span class="n">MultivariateNormalTri</span><span class="p">:</span>

    <span class="n">map_latent_dist</span> <span class="o">=</span> <span class="n">posterior</span><span class="p">(</span><span class="n">map_estimate</span><span class="p">,</span> <span class="n">D</span><span class="p">)(</span><span class="n">test_inputs</span><span class="p">)</span>

    <span class="n">Kxt</span> <span class="o">=</span> <span class="n">cross_covariance</span><span class="p">(</span><span class="n">map_estimate</span><span class="p">[</span><span class="s2">&quot;kernel&quot;</span><span class="p">],</span> <span class="n">x</span><span class="p">,</span> <span class="n">test_inputs</span><span class="p">)</span>
    <span class="n">Kxx</span> <span class="o">=</span> <span class="n">gram</span><span class="p">(</span><span class="n">map_estimate</span><span class="p">[</span><span class="s2">&quot;kernel&quot;</span><span class="p">],</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">Kxx</span> <span class="o">+=</span> <span class="n">I</span><span class="p">(</span><span class="n">D</span><span class="o">.</span><span class="n">n</span><span class="p">)</span> <span class="o">*</span> <span class="n">jitter</span>
    <span class="n">Lx</span> <span class="o">=</span> <span class="n">Kxx</span><span class="o">.</span><span class="n">to_root</span><span class="p">()</span>

    <span class="c1"># Lx⁻¹ Kxt</span>
    <span class="n">Lx_inv_Ktx</span> <span class="o">=</span> <span class="n">Lx</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">Kxt</span><span class="p">)</span>

    <span class="c1"># Kxx⁻¹ Kxt</span>
    <span class="n">Kxx_inv_Ktx</span> <span class="o">=</span> <span class="n">Lx</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">Lx_inv_Ktx</span><span class="p">)</span>

    <span class="c1"># Ktx Kxx⁻¹[ H⁻¹ ] Kxx⁻¹ Kxt</span>
    <span class="n">laplace_cov_term</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">Kxx_inv_Ktx</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">H_inv</span><span class="p">),</span> <span class="n">Kxx_inv_Ktx</span><span class="p">)</span>

    <span class="n">mean</span> <span class="o">=</span> <span class="n">map_latent_dist</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">covariance</span> <span class="o">=</span> <span class="n">map_latent_dist</span><span class="o">.</span><span class="n">covariance</span><span class="p">()</span> <span class="o">+</span> <span class="n">laplace_cov_term</span>
    <span class="n">L</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">covariance</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dx</span><span class="o">.</span><span class="n">MultivariateNormalTri</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">mean</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()),</span> <span class="n">L</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>From this we can construct the predictive distribution at the test points.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">laplace_latent_dist</span> <span class="o">=</span> <span class="n">construct_laplace</span><span class="p">(</span><span class="n">xtest</span><span class="p">)</span>
<span class="n">predictive_dist</span> <span class="o">=</span> <span class="n">likelihood</span><span class="p">(</span><span class="n">map_estimate</span><span class="p">,</span> <span class="n">laplace_latent_dist</span><span class="p">)</span>

<span class="n">predictive_mean</span> <span class="o">=</span> <span class="n">predictive_dist</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">predictive_std</span> <span class="o">=</span> <span class="n">predictive_dist</span><span class="o">.</span><span class="n">stddev</span><span class="p">()</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Observations&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:red&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xtest</span><span class="p">,</span> <span class="n">predictive_mean</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Predictive mean&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
    <span class="n">xtest</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(),</span>
    <span class="n">predictive_mean</span> <span class="o">-</span> <span class="n">predictive_std</span><span class="p">,</span>
    <span class="n">predictive_mean</span> <span class="o">+</span> <span class="n">predictive_std</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Two sigma&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">xtest</span><span class="p">,</span>
    <span class="n">predictive_mean</span> <span class="o">-</span> <span class="n">predictive_std</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span>
    <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span>
    <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">xtest</span><span class="p">,</span>
    <span class="n">predictive_mean</span> <span class="o">+</span> <span class="n">predictive_std</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span>
    <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;--&quot;</span><span class="p">,</span>
    <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x285348940&gt;
</pre></div>
</div>
<img alt="../_images/classification_20_1.png" src="../_images/classification_20_1.png" />
</div>
</div>
<p>However, the Laplace approximation is still limited by considering information about the posterior at a single location. On the other hand, through approximate sampling, MCMC methods allow us to learn all information about the posterior distribution.</p>
</section>
<section id="mcmc-inference">
<h2>MCMC inference<a class="headerlink" href="#mcmc-inference" title="Permalink to this heading">#</a></h2>
<p>At the high level, an MCMC sampler works by starting at an initial position and drawing a sample from a cheap-to-simulate distribution known as the <em>proposal</em>. The next step is to determine whether this sample could be considered a draw from the posterior. We accomplish this using an <em>acceptance probability</em> determined via the sampler’s <em>transition kernel</em> which depends on the current position and the unnormalised target posterior distribution. If the new sample is more <em>likely</em>, we accept it; otherwise, we reject it and stay in our current position. Repeating these steps results in a Markov chain (a random sequence that depends only on the last state) whose stationary distribution (the long-run empirical distribution of the states visited) is the posterior. For a gentle introduction, see the first chapter of <a class="reference external" href="https://www.mcmchandbook.net/HandbookChapter1.pdf">A Handbook of Markov Chain Monte Carlo</a>.</p>
<section id="mcmc-through-blackjax">
<h3>MCMC through BlackJax<a class="headerlink" href="#mcmc-through-blackjax" title="Permalink to this heading">#</a></h3>
<p>Rather than implementing a suite of MCMC samplers, GPJax relies on MCMC-specific libraries for sampling functionality. We focus on <a class="reference external" href="https://github.com/blackjax-devs/blackjax/">BlackJax</a> in this notebook, which we recommend adopting for general applications. However, we also support TensorFlow Probability as demonstrated in the <a class="reference external" href="https://gpjax.readthedocs.io/en/latest/nbs/tfp_integration.html">TensorFlow Probability Integration notebook</a>.</p>
<p>We’ll use the No U-Turn Sampler (NUTS) implementation given in BlackJax for sampling. For the interested reader, NUTS is a Hamiltonian Monte Carlo sampling scheme where the number of leapfrog integration steps is computed at each step of the change according to the NUTS algorithm. In general, samplers constructed under this framework are very efficient.</p>
<p>We begin by generating <em>sensible</em> initial positions for our sampler before defining an inference loop and sampling 500 values from our Markov chain. In practice, drawing more samples will be necessary.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Adapted from BlackJax&#39;s introduction notebook.</span>
<span class="n">num_adapt</span> <span class="o">=</span> <span class="mi">500</span>
<span class="n">num_samples</span> <span class="o">=</span> <span class="mi">500</span>

<span class="n">params</span><span class="p">,</span> <span class="n">trainables</span><span class="p">,</span> <span class="n">bijectors</span> <span class="o">=</span> <span class="n">gpx</span><span class="o">.</span><span class="n">initialise</span><span class="p">(</span><span class="n">posterior</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span><span class="o">.</span><span class="n">unpack</span><span class="p">()</span>
<span class="n">mll</span> <span class="o">=</span> <span class="n">posterior</span><span class="o">.</span><span class="n">marginal_log_likelihood</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">negative</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">unconstrained_mll</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">jit</span><span class="p">(</span><span class="k">lambda</span> <span class="n">params</span><span class="p">:</span> <span class="n">mll</span><span class="p">(</span><span class="n">gpx</span><span class="o">.</span><span class="n">constrain</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">bijectors</span><span class="p">)))</span>

<span class="n">adapt</span> <span class="o">=</span> <span class="n">blackjax</span><span class="o">.</span><span class="n">window_adaptation</span><span class="p">(</span>
    <span class="n">blackjax</span><span class="o">.</span><span class="n">nuts</span><span class="p">,</span> <span class="n">unconstrained_mll</span><span class="p">,</span> <span class="n">num_adapt</span><span class="p">,</span> <span class="n">target_acceptance_rate</span><span class="o">=</span><span class="mf">0.65</span>
<span class="p">)</span>

<span class="c1"># Initialise the chain</span>
<span class="n">unconstrained_params</span> <span class="o">=</span> <span class="n">gpx</span><span class="o">.</span><span class="n">unconstrain</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">bijectors</span><span class="p">)</span>
<span class="n">last_state</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">adapt</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">unconstrained_params</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">inference_loop</span><span class="p">(</span><span class="n">rng_key</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">initial_state</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">one_step</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">rng_key</span><span class="p">):</span>
        <span class="n">state</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">kernel</span><span class="p">(</span><span class="n">rng_key</span><span class="p">,</span> <span class="n">state</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">state</span><span class="p">,</span> <span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">info</span><span class="p">)</span>

    <span class="n">keys</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">rng_key</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="p">(</span><span class="n">states</span><span class="p">,</span> <span class="n">infos</span><span class="p">)</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">scan</span><span class="p">(</span><span class="n">one_step</span><span class="p">,</span> <span class="n">initial_state</span><span class="p">,</span> <span class="n">keys</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">states</span><span class="p">,</span> <span class="n">infos</span>


<span class="c1"># Sample from the posterior distribution</span>
<span class="n">states</span><span class="p">,</span> <span class="n">infos</span> <span class="o">=</span> <span class="n">inference_loop</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">kernel</span><span class="p">,</span> <span class="n">last_state</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="sampler-efficiency">
<h3>Sampler efficiency<a class="headerlink" href="#sampler-efficiency" title="Permalink to this heading">#</a></h3>
<p>BlackJax gives us easy access to our sampler’s efficiency through metrics such as the sampler’s <em>acceptance probability</em> (the number of times that our chain accepted a proposed sample, divided by the total number of steps run by the chain). For NUTS and Hamiltonian Monte Carlo sampling, we typically seek an acceptance rate of 60-70% to strike the right balance between having a chain which is <em>stuck</em> and rarely moves versus a chain that is too jumpy with frequent small steps.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">acceptance_rate</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">infos</span><span class="o">.</span><span class="n">acceptance_probability</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Acceptance rate: </span><span class="si">{</span><span class="n">acceptance_rate</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Acceptance rate: 0.82
</pre></div>
</div>
</div>
</div>
<p>Our acceptance rate is slightly too large, prompting an examination of the chain’s trace plots. A well-mixing chain will have very few (if any) flat spots in its trace plot whilst also not having too many steps in the same direction. In addition to the model’s hyperparameters, there will be 500 samples for each of the 100 latent function values in the <code class="docutils literal notranslate"><span class="pre">states.position</span></code> dictionary. We depict the chains that correspond to the model hyperparameters and the first value of the latent function for brevity.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax0</span><span class="p">,</span> <span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">ncols</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">tight_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax0</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">states</span><span class="o">.</span><span class="n">position</span><span class="p">[</span><span class="s2">&quot;kernel&quot;</span><span class="p">][</span><span class="s2">&quot;lengthscale&quot;</span><span class="p">])</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">states</span><span class="o">.</span><span class="n">position</span><span class="p">[</span><span class="s2">&quot;kernel&quot;</span><span class="p">][</span><span class="s2">&quot;variance&quot;</span><span class="p">])</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">states</span><span class="o">.</span><span class="n">position</span><span class="p">[</span><span class="s2">&quot;latent&quot;</span><span class="p">][:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:])</span>
<span class="n">ax0</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Kernel Lengthscale&quot;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Kernel Variance&quot;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s2">&quot;Latent Function (index = 1)&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0.5, 1.0, &#39;Latent Function (index = 1)&#39;)
</pre></div>
</div>
<img alt="../_images/classification_27_1.png" src="../_images/classification_27_1.png" />
</div>
</div>
</section>
</section>
<section id="prediction">
<h2>Prediction<a class="headerlink" href="#prediction" title="Permalink to this heading">#</a></h2>
<p>Having obtained samples from the posterior, we draw ten instances from our model’s predictive distribution per MCMC sample. Using these draws, we will be able to compute credible values and expected values under our posterior distribution.</p>
<p>An ideal Markov chain would have samples completely uncorrelated with their neighbours after a single lag. However, in practice, correlations often exist within our chain’s sample set. A commonly used technique to try and reduce this correlation is <em>thinning</em> whereby we select every <span class="math notranslate nohighlight">\(n\)</span>th sample where <span class="math notranslate nohighlight">\(n\)</span> is the minimum lag length at which we believe the samples are uncorrelated. Although further analysis of the chain’s autocorrelation is required to find appropriate thinning factors, we employ a thin factor of 10 for demonstration purposes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">thin_factor</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">samples</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">,</span> <span class="n">thin_factor</span><span class="p">):</span>
    <span class="n">ps</span> <span class="o">=</span> <span class="n">gpx</span><span class="o">.</span><span class="n">parameters</span><span class="o">.</span><span class="n">copy_dict_structure</span><span class="p">(</span><span class="n">params</span><span class="p">)</span>
    <span class="n">ps</span><span class="p">[</span><span class="s2">&quot;kernel&quot;</span><span class="p">][</span><span class="s2">&quot;lengthscale&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">states</span><span class="o">.</span><span class="n">position</span><span class="p">[</span><span class="s2">&quot;kernel&quot;</span><span class="p">][</span><span class="s2">&quot;lengthscale&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
    <span class="n">ps</span><span class="p">[</span><span class="s2">&quot;kernel&quot;</span><span class="p">][</span><span class="s2">&quot;variance&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">states</span><span class="o">.</span><span class="n">position</span><span class="p">[</span><span class="s2">&quot;kernel&quot;</span><span class="p">][</span><span class="s2">&quot;variance&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">]</span>
    <span class="n">ps</span><span class="p">[</span><span class="s2">&quot;latent&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">states</span><span class="o">.</span><span class="n">position</span><span class="p">[</span><span class="s2">&quot;latent&quot;</span><span class="p">][</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
    <span class="n">ps</span> <span class="o">=</span> <span class="n">gpx</span><span class="o">.</span><span class="n">constrain</span><span class="p">(</span><span class="n">ps</span><span class="p">,</span> <span class="n">bijectors</span><span class="p">)</span>

    <span class="n">latent_dist</span> <span class="o">=</span> <span class="n">posterior</span><span class="p">(</span><span class="n">ps</span><span class="p">,</span> <span class="n">D</span><span class="p">)(</span><span class="n">xtest</span><span class="p">)</span>
    <span class="n">predictive_dist</span> <span class="o">=</span> <span class="n">likelihood</span><span class="p">(</span><span class="n">ps</span><span class="p">,</span> <span class="n">latent_dist</span><span class="p">)</span>
    <span class="n">samples</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">predictive_dist</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">key</span><span class="p">,</span> <span class="n">sample_shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,)))</span>

<span class="n">samples</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">vstack</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>

<span class="n">lower_ci</span><span class="p">,</span> <span class="n">upper_ci</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">percentile</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">2.5</span><span class="p">,</span> <span class="mf">97.5</span><span class="p">]),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">expected_val</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Finally, we end this tutorial by plotting the predictions obtained from our model against the observed data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="mi">5</span><span class="p">),</span> <span class="n">tight_layout</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">markersize</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:red&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Observations&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.7</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
    <span class="n">xtest</span><span class="p">,</span> <span class="n">expected_val</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Predicted mean&quot;</span><span class="p">,</span> <span class="n">zorder</span><span class="o">=</span><span class="mi">1</span>
<span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span>
    <span class="n">xtest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span>
    <span class="n">lower_ci</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span>
    <span class="n">upper_ci</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
    <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">,</span>
    <span class="n">label</span><span class="o">=</span><span class="s2">&quot;95% CI&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.collections.PolyCollection at 0x292227400&gt;
</pre></div>
</div>
<img alt="../_images/classification_31_1.png" src="../_images/classification_31_1.png" />
</div>
</div>
</section>
<section id="system-configuration">
<h2>System configuration<a class="headerlink" href="#system-configuration" title="Permalink to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">load_ext</span> watermark
<span class="o">%</span><span class="k">watermark</span> -n -u -v -iv -w -a &quot;Thomas Pinder &amp; Daniel Dodd&quot;
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Author: Thomas Pinder &amp; Daniel Dodd

Last updated: Sat Jan 14 2023

Python implementation: CPython
Python version       : 3.9.15
IPython version      : 8.8.0

optax     : 0.1.4
matplotlib: 3.3.3
blackjax  : 0.9.6
jax       : 0.4.1
gpjax     : 0.5.8
jaxkern   : 0.0.4
distrax   : 0.1.2

Watermark: 2.3.1
</pre></div>
</div>
</div>
</div>
</section>
</section>


<script type="application/vnd.jupyter.widget-state+json">
{"state": {"ff566e8c7923489f8369becbfccb8fd9": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "fd720323233c48e688a6407ca9bf77ca": {"model_name": "ProgressStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "b1d6c97fa4e941f483bcec997b620768": {"model_name": "FloatProgressModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_ff566e8c7923489f8369becbfccb8fd9", "max": 1000.0, "min": 0.0, "orientation": "horizontal", "style": "IPY_MODEL_fd720323233c48e688a6407ca9bf77ca", "value": 1000.0}}, "0fd6cb1d12c944dd80dd55756b519d0a": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "f8dea8ecdd4a46feb0edcdd095df1da7": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "e33dc3f72c7a4846a71c23ab2f0a54fb": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_0fd6cb1d12c944dd80dd55756b519d0a", "placeholder": "\u200b", "style": "IPY_MODEL_f8dea8ecdd4a46feb0edcdd095df1da7", "value": "100%"}}, "3f78940ce6ed49f7892f2901d32273d6": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "758a11455f5045088776624e43d6e60c": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "3d5976723dae4d93b7e89292032bebcc": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_3f78940ce6ed49f7892f2901d32273d6", "placeholder": "\u200b", "style": "IPY_MODEL_758a11455f5045088776624e43d6e60c", "value": " 1000/1000 [00:01&lt;00:00, 919.66it/s, Objective=102.67]"}}, "6283a7f6596848a1915b8ca7e98c25b9": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "7aaeec6ff8274fa5aa85e307d5004579": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_e33dc3f72c7a4846a71c23ab2f0a54fb", "IPY_MODEL_b1d6c97fa4e941f483bcec997b620768", "IPY_MODEL_3d5976723dae4d93b7e89292032bebcc"], "layout": "IPY_MODEL_6283a7f6596848a1915b8ca7e98c25b9"}}}, "version_major": 2, "version_minor": 0}
</script>


    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./nbs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="regression.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Regression</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="collapsed_vi.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Sparse Gaussian Process Regression</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The Jupyter Book Community<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>