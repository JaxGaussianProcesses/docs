
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Sparse Stochastic Variational Inference &#8212; My sample book</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Gaussian Processes Barycentres" href="barycentres.html" />
    <link rel="prev" title="Sparse Gaussian Process Regression" href="collapsed_vi.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">My sample book</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Welcome to your Jupyter Book
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="intro_to_gps.html">
   New to Gaussian Processes?
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="regression.html">
   Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="classification.html">
   Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="collapsed_vi.html">
   Sparse Gaussian Process Regression
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Sparse Stochastic Variational Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="barycentres.html">
   Gaussian Processes Barycentres
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="haiku.html">
   Deep Kernel Learning
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="kernels.html">
   Kernel Guide
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="graph_kernels.html">
   Graph Kernels
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="tfp_integration.html">
   TensorFlow Probability Integration
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="natgrads.html">
   Natural Gradients
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="yacht.html">
   UCI Data Benchmarking
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/docs/nbs/uncollapsed_vi.py"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/executablebooks/jupyter-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fnbs/uncollapsed_vi.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/nbs/uncollapsed_vi.pct.py"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.pct.py</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dataset">
   Dataset
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sparse-gps-via-inducing-inputs">
   Sparse GPs via inducing inputs
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#defining-the-variational-process">
   Defining the variational process
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inference">
   Inference
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#evidence-lower-bound">
     Evidence lower bound
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mini-batching">
     Mini-batching
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#predictions">
   Predictions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#custom-transformations">
   Custom transformations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#system-configuration">
   System configuration
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Sparse Stochastic Variational Inference</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dataset">
   Dataset
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sparse-gps-via-inducing-inputs">
   Sparse GPs via inducing inputs
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#defining-the-variational-process">
   Defining the variational process
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#inference">
   Inference
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#evidence-lower-bound">
     Evidence lower bound
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mini-batching">
     Mini-batching
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#predictions">
   Predictions
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#custom-transformations">
   Custom transformations
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#system-configuration">
   System configuration
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="sparse-stochastic-variational-inference">
<h1>Sparse Stochastic Variational Inference<a class="headerlink" href="#sparse-stochastic-variational-inference" title="Permalink to this headline">#</a></h1>
<p>In this notebook we demonstrate how to implement sparse variational Gaussian processes (SVGPs) of <strong data-cite="hensman2013gaussian">Hensman et al. (2013)</strong>; <strong data-cite="hensman2015gaussian">Hensman et al. (2015)</strong>. In particular, this approximation framework provides a tractable option for working with non-conjugate Gaussian processes with more than ~5000 data points. However, for conjugate models of less than 5000 data points, we recommend using the marginal log-likelihood approach presented in the <a class="reference external" href="https://gpjax.readthedocs.io/en/latest/nbs/regression.html">regression notebook</a>. Though we illustrate SVGPs here with a conjugate regression example, the same GPJax code works for general likelihoods, such as a Bernoulli for classification.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="kn">import</span> <span class="nn">jax.random</span> <span class="k">as</span> <span class="nn">jr</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">optax</span> <span class="k">as</span> <span class="nn">ox</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">jit</span>
<span class="kn">from</span> <span class="nn">jax.config</span> <span class="kn">import</span> <span class="n">config</span>
<span class="kn">from</span> <span class="nn">jaxutils</span> <span class="kn">import</span> <span class="n">Dataset</span>
<span class="kn">import</span> <span class="nn">jaxkern</span> <span class="k">as</span> <span class="nn">jk</span>

<span class="kn">import</span> <span class="nn">tensorflow_probability.substrates.jax</span> <span class="k">as</span> <span class="nn">tfp</span>

<span class="n">tfb</span> <span class="o">=</span> <span class="n">tfp</span><span class="o">.</span><span class="n">bijectors</span>

<span class="kn">import</span> <span class="nn">distrax</span> <span class="k">as</span> <span class="nn">dx</span>
<span class="kn">import</span> <span class="nn">gpjax</span> <span class="k">as</span> <span class="nn">gpx</span>
<span class="kn">from</span> <span class="nn">gpjax.config</span> <span class="kn">import</span> <span class="n">get_global_config</span><span class="p">,</span> <span class="n">reset_global_config</span>

<span class="c1"># Enable Float64 for more stable matrix inversions.</span>
<span class="n">config</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="s2">&quot;jax_enable_x64&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>
<span class="n">key</span> <span class="o">=</span> <span class="n">jr</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">123</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)
</pre></div>
</div>
</div>
</div>
<section id="dataset">
<h2>Dataset<a class="headerlink" href="#dataset" title="Permalink to this headline">#</a></h2>
<p>With the necessary modules imported, we simulate a dataset <span class="math notranslate nohighlight">\(\mathcal{D} = (\boldsymbol{x}, \boldsymbol{y}) = \{(x_i, y_i)\}_{i=1}^{5000}\)</span> with inputs <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> sampled uniformly on <span class="math notranslate nohighlight">\((-5, 5)\)</span> and corresponding binary outputs</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{y} \sim \mathcal{N} \left(\sin(4 * \boldsymbol{x}) + \sin(2 * \boldsymbol{x}), \textbf{I} * (0.2)^{2} \right).\]</div>
<p>We store our data <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> as a GPJax <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> and create test inputs for later.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">noise</span> <span class="o">=</span> <span class="mf">0.2</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">jr</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">key</span><span class="o">=</span><span class="n">key</span><span class="p">,</span> <span class="n">minval</span><span class="o">=-</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">maxval</span><span class="o">=</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">n</span><span class="p">,))</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">jnp</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
<span class="n">signal</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">signal</span> <span class="o">+</span> <span class="n">jr</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">signal</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">*</span> <span class="n">noise</span>

<span class="n">D</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">(</span><span class="n">X</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">)</span>

<span class="n">xtest</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">5.5</span><span class="p">,</span> <span class="mf">5.5</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="sparse-gps-via-inducing-inputs">
<h2>Sparse GPs via inducing inputs<a class="headerlink" href="#sparse-gps-via-inducing-inputs" title="Permalink to this headline">#</a></h2>
<p>Despite their endowment with elegant theoretical properties, GPs are burdened with prohibitive <span class="math notranslate nohighlight">\(\mathcal{O}(n^3)\)</span> inference and <span class="math notranslate nohighlight">\(\mathcal{O}(n^2)\)</span> memory costs in the number of data points <span class="math notranslate nohighlight">\(n\)</span> due to the necessity of computing inverses and determinants of the kernel Gram matrix <span class="math notranslate nohighlight">\(\mathbf{K}_{\boldsymbol{x}\boldsymbol{x}}\)</span> during inference and hyperparameter learning.
Sparse GPs seek to resolve tractability through low-rank approximations.</p>
<p>Their name originates with the idea of using subsets of the data to approximate the kernel matrix, with <em>sparseness</em> occurring through the selection of the data points.
Given inputs <span class="math notranslate nohighlight">\(\boldsymbol{x}\)</span> and outputs <span class="math notranslate nohighlight">\(\boldsymbol{y}\)</span> the task was to select an <span class="math notranslate nohighlight">\(m&lt;n\)</span> lower-dimensional dataset <span class="math notranslate nohighlight">\((\boldsymbol{z},\boldsymbol{\tilde{y}}) \subset (\boldsymbol{x}, \boldsymbol{y})\)</span> to train a Gaussian process on instead.
By generalising the set of selected points <span class="math notranslate nohighlight">\(\boldsymbol{z}\)</span>, known as <em>inducing inputs</em>, to remove the restriction of being part of the dataset,
we can arrive at a flexible low-rank approximation framework of the model using functions of <span class="math notranslate nohighlight">\(\mathbf{K}_{\boldsymbol{z}\boldsymbol{z}}\)</span> to replace the true covariance matrix <span class="math notranslate nohighlight">\(\mathbf{K}_{\boldsymbol{x}\boldsymbol{x}}\)</span> at significantly lower costs. For example, <strong data-cite="quinonero-candela2005gaussian"></strong> review many popular approximation schemes in this vein. However, because the model and the approximation are intertwined, assigning performance and faults to one or the other becomes tricky.</p>
<p>On the other hand, sparse variational Gaussian processes (SVGPs) <a class="reference external" href="https://www.secondmind.ai/labs/sparse-gps-approximate-the-posterior-not-the-model/">approximate the posterior, not the model</a>.
These provide a low-rank approximation scheme via variational inference. Here we posit a family of densities parameterised by “variational parameters”.
We then seek to find the closest family member to the posterior by minimising the Kullback-Leibler divergence over the variational parameters.
The fitted variational density then serves as a proxy for the exact posterior.
This procedure makes variational methods efficiently solvable via off-the-shelf optimisation techniques whilst retaining the true-underlying model.
Furthermore, SVGPs offer further cost reductions with mini-batch stochastic gradient descent  <strong data-cite="hensman2013gaussian"></strong> and address non-conjugacy <strong data-cite="hensman2015gaussian"></strong>.
We show a cost comparison between the approaches below, where <span class="math notranslate nohighlight">\(b\)</span> is the mini-batch size.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p>GPs</p></th>
<th class="head"><p>sparse GPs</p></th>
<th class="head"><p>SVGP</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Inference cost</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{O}(n^3)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{O}(n m^2)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{O}(b m^2 + m^3)\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>Memory cost</p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{O}(n^2)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{O}(n m)\)</span></p></td>
<td><p><span class="math notranslate nohighlight">\(\mathcal{O}(b m + m^2)\)</span></p></td>
</tr>
</tbody>
</table>
<p>To apply SVGP inference to our dataset, we begin by initialising <span class="math notranslate nohighlight">\(m = 50\)</span> equally spaced inducing inputs <span class="math notranslate nohighlight">\(\boldsymbol{z}\)</span> across our observed data’s support. These are depicted below via horizontal black lines.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">z</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mf">5.0</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xtest</span><span class="p">,</span> <span class="n">f</span><span class="p">(</span><span class="n">xtest</span><span class="p">))</span>
<span class="p">[</span><span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">z_i</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">z_i</span> <span class="ow">in</span> <span class="n">z</span><span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/uncollapsed_vi_5_0.png" src="../_images/uncollapsed_vi_5_0.png" />
</div>
</div>
<p>The inducing inputs will summarise our dataset, and since they are treated as variational parameters, their locations will be optimised. The next step to SVGP is to define a variational family.</p>
</section>
<section id="defining-the-variational-process">
<h2>Defining the variational process<a class="headerlink" href="#defining-the-variational-process" title="Permalink to this headline">#</a></h2>
<p>We begin by considering the form of the posterior distribution for all function values <span class="math notranslate nohighlight">\(f(\cdot)\)</span></p>
<p>\begin{align}
p(f(\cdot) | \mathcal{D}) = \int p(f(\cdot)|f(\boldsymbol{x})) p(f(\boldsymbol{x})|\mathcal{D}) \text{d}f(\boldsymbol{x}). \qquad (\dagger)
\end{align}</p>
<p>To arrive at an approximation framework, we assume some redundancy in the data. Instead of predicting <span class="math notranslate nohighlight">\(f(\cdot)\)</span> with function values at the datapoints <span class="math notranslate nohighlight">\(f(\boldsymbol{x})\)</span>, we assume this can be achieved with only function values at <span class="math notranslate nohighlight">\(m\)</span> inducing inputs <span class="math notranslate nohighlight">\(\boldsymbol{z}\)</span></p>
<div class="math notranslate nohighlight">
\[ p(f(\cdot) | \mathcal{D}) \approx \int p(f(\cdot)|f(\boldsymbol{z})) p(f(\boldsymbol{z})|\mathcal{D}) \text{d}f(\boldsymbol{z}). \qquad (\star) \]</div>
<p>This lower dimensional integral results in computational savings in the model’s predictive component from <span class="math notranslate nohighlight">\(p(f(\cdot)|f(\boldsymbol{x}))\)</span> to <span class="math notranslate nohighlight">\(p(f(\cdot)|f(\boldsymbol{z}))\)</span> where inverting <span class="math notranslate nohighlight">\(\mathbf{K}_{\boldsymbol{x}\boldsymbol{x}}\)</span> is replaced with inverting <span class="math notranslate nohighlight">\(\mathbf{K}_{\boldsymbol{z}\boldsymbol{z}}\)</span>.
However, since we did not observe our data <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> at <span class="math notranslate nohighlight">\(\boldsymbol{z}\)</span> we ask, what exactly is the posterior <span class="math notranslate nohighlight">\(p(f(\boldsymbol{z})|\mathcal{D})\)</span>?</p>
<p>Notice this is simply obtained by substituting <span class="math notranslate nohighlight">\(\boldsymbol{z}\)</span> into <span class="math notranslate nohighlight">\((\dagger)\)</span>, but we arrive back at square one with computing the expensive integral. To side-step this, we consider replacing <span class="math notranslate nohighlight">\(p(f(\boldsymbol{z})|\mathcal{D})\)</span> in <span class="math notranslate nohighlight">\((\star)\)</span> with a cheap-to-compute approximate distribution <span class="math notranslate nohighlight">\(q(f(\boldsymbol{z}))\)</span></p>
<div class="math notranslate nohighlight">
\[ q(f(\cdot)) = \int p(f(\cdot)|f(\boldsymbol{z})) q(f(\boldsymbol{z})) \text{d}f(\boldsymbol{z}). \qquad (\times) \]</div>
<p>To measure the quality of the approximation, we consider the Kullback-Leibler divergence <span class="math notranslate nohighlight">\(\operatorname{KL}(\cdot || \cdot)\)</span> from our approximate process <span class="math notranslate nohighlight">\(q(f(\cdot))\)</span> to the true process <span class="math notranslate nohighlight">\(p(f(\cdot)|\mathcal{D})\)</span>. By parametrising <span class="math notranslate nohighlight">\(q(f(\boldsymbol{z}))\)</span> over a variational family of distributions, we can optimise Kullback-Leibler divergence with respect to the variational parameters. Moreover, since inducing input locations <span class="math notranslate nohighlight">\(\boldsymbol{z}\)</span> augment the model, they themselves can be treated as variational parameters without altering the true underlying model <span class="math notranslate nohighlight">\(p(f(\boldsymbol{z})|\mathcal{D})\)</span>. This is exactly what gives SVGPs great flexibility whilst retaining robustness to overfitting.</p>
<p>It is popular to elect a Gaussian variational distribution <span class="math notranslate nohighlight">\(q(f(\boldsymbol{z})) = \mathcal{N}(f(\boldsymbol{z}); \mathbf{m}, \mathbf{S})\)</span> with parameters <span class="math notranslate nohighlight">\(\{\boldsymbol{z}, \mathbf{m}, \mathbf{S}\}\)</span>, since conjugacy is provided between <span class="math notranslate nohighlight">\(q(f(\boldsymbol{z}))\)</span> and <span class="math notranslate nohighlight">\(p(f(\cdot)|f(\boldsymbol{z}))\)</span> so that the resulting variational process <span class="math notranslate nohighlight">\(q(f(\cdot))\)</span> is a GP. We can implement this in GPJax by the following.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">likelihood</span> <span class="o">=</span> <span class="n">gpx</span><span class="o">.</span><span class="n">Gaussian</span><span class="p">(</span><span class="n">num_datapoints</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
<span class="n">prior</span> <span class="o">=</span> <span class="n">gpx</span><span class="o">.</span><span class="n">Prior</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="n">jk</span><span class="o">.</span><span class="n">RBF</span><span class="p">())</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">prior</span> <span class="o">*</span> <span class="n">likelihood</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">gpx</span><span class="o">.</span><span class="n">VariationalGaussian</span><span class="p">(</span><span class="n">prior</span><span class="o">=</span><span class="n">prior</span><span class="p">,</span> <span class="n">inducing_inputs</span><span class="o">=</span><span class="n">z</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Here, the variational process <span class="math notranslate nohighlight">\(q(\cdot)\)</span> depends on the prior through <span class="math notranslate nohighlight">\(p(f(\cdot)|f(\boldsymbol{z}))\)</span> in <span class="math notranslate nohighlight">\((\times)\)</span>.</p>
<p>We combine our true and approximate posterior Gaussian processes into an <code class="docutils literal notranslate"><span class="pre">StochasticVI</span></code> object to define the variational strategy that we will adopt in the forthcoming inference.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">svgp</span> <span class="o">=</span> <span class="n">gpx</span><span class="o">.</span><span class="n">StochasticVI</span><span class="p">(</span><span class="n">posterior</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">variational_family</span><span class="o">=</span><span class="n">q</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="inference">
<h2>Inference<a class="headerlink" href="#inference" title="Permalink to this headline">#</a></h2>
<section id="evidence-lower-bound">
<h3>Evidence lower bound<a class="headerlink" href="#evidence-lower-bound" title="Permalink to this headline">#</a></h3>
<p>With our model defined, we seek to infer the optimal inducing inputs <span class="math notranslate nohighlight">\(\boldsymbol{z}\)</span>, variational mean <span class="math notranslate nohighlight">\(\mathbf{m}\)</span> and covariance <span class="math notranslate nohighlight">\(\mathbf{S}\)</span> that define our approximate posterior. To achieve this, we maximise the evidence lower bound (ELBO) with respect to <span class="math notranslate nohighlight">\(\{\boldsymbol{z}, \mathbf{m}, \mathbf{S} \}\)</span>, a proxy for minimising the Kullback-Leibler divergence. Moreover, as hinted by its name, the ELBO is a lower bound to the marginal log-likelihood, providing a tractable objective to optimise the model’s hyperparameters akin to the conjugate setting. For further details on this, see Sections 3.1 and 4.1 of the excellent review paper <strong data-cite="leibfried2020tutorial"></strong>.</p>
<p>Since Optax’s optimisers work to minimise functions, to maximise the ELBO we return its negative.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">negative_elbo</span> <span class="o">=</span> <span class="n">jit</span><span class="p">(</span><span class="n">svgp</span><span class="o">.</span><span class="n">elbo</span><span class="p">(</span><span class="n">D</span><span class="p">,</span> <span class="n">negative</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="mini-batching">
<h3>Mini-batching<a class="headerlink" href="#mini-batching" title="Permalink to this headline">#</a></h3>
<p>Despite introducing inducing inputs into our model, inference can still be intractable with large datasets. To circumvent this, optimisation can be done using stochastic mini-batches.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">reset_global_config</span><span class="p">()</span>
<span class="n">parameter_state</span> <span class="o">=</span> <span class="n">gpx</span><span class="o">.</span><span class="n">initialise</span><span class="p">(</span><span class="n">svgp</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
<span class="n">optimiser</span> <span class="o">=</span> <span class="n">ox</span><span class="o">.</span><span class="n">adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="n">inference_state</span> <span class="o">=</span> <span class="n">gpx</span><span class="o">.</span><span class="n">fit_batches</span><span class="p">(</span>
    <span class="n">objective</span><span class="o">=</span><span class="n">negative_elbo</span><span class="p">,</span>
    <span class="n">parameter_state</span><span class="o">=</span><span class="n">parameter_state</span><span class="p">,</span>
    <span class="n">train_data</span><span class="o">=</span><span class="n">D</span><span class="p">,</span>
    <span class="n">optax_optim</span><span class="o">=</span><span class="n">optimiser</span><span class="p">,</span>
    <span class="n">num_iters</span><span class="o">=</span><span class="mi">3000</span><span class="p">,</span>
    <span class="n">key</span><span class="o">=</span><span class="n">jr</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">42</span><span class="p">),</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">learned_params</span><span class="p">,</span> <span class="n">training_history</span> <span class="o">=</span> <span class="n">inference_state</span><span class="o">.</span><span class="n">unpack</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/tmp/ipykernel_3143/3641125512.py:1: DeprecatedWarning: reset_global_config is deprecated as of 0.5.6 and will be removed in 0.6.0. Use method from jaxutils.config instead.
  reset_global_config()
</pre></div>
</div>
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "931074abc78642488b89d0f79068a02c"}
</script></div>
</div>
</section>
</section>
<section id="predictions">
<h2>Predictions<a class="headerlink" href="#predictions" title="Permalink to this headline">#</a></h2>
<p>With optimisation complete, we can use our inferred parameter set to make predictions at novel inputs akin
to all other models within GPJax on our variational process object <span class="math notranslate nohighlight">\(q(\cdot)\)</span> (for example, see the <a class="reference external" href="https://gpjax.readthedocs.io/en/latest/nbs/regression.html">regression notebook</a>).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">latent_dist</span> <span class="o">=</span> <span class="n">q</span><span class="p">(</span><span class="n">learned_params</span><span class="p">)(</span><span class="n">xtest</span><span class="p">)</span>
<span class="n">predictive_dist</span> <span class="o">=</span> <span class="n">likelihood</span><span class="p">(</span><span class="n">learned_params</span><span class="p">,</span> <span class="n">latent_dist</span><span class="p">)</span>

<span class="n">meanf</span> <span class="o">=</span> <span class="n">predictive_dist</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="n">predictive_dist</span><span class="o">.</span><span class="n">stddev</span><span class="p">()</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.15</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Training Data&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:gray&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xtest</span><span class="p">,</span> <span class="n">meanf</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Posterior mean&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">xtest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">meanf</span> <span class="o">-</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">meanf</span> <span class="o">+</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="p">[</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">z_i</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">z_i</span> <span class="ow">in</span> <span class="n">learned_params</span><span class="p">[</span><span class="s2">&quot;variational_family&quot;</span><span class="p">][</span><span class="s2">&quot;inducing_inputs&quot;</span><span class="p">]</span>
<span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/uncollapsed_vi_17_0.png" src="../_images/uncollapsed_vi_17_0.png" />
</div>
</div>
</section>
<section id="custom-transformations">
<h2>Custom transformations<a class="headerlink" href="#custom-transformations" title="Permalink to this headline">#</a></h2>
<p>To train a covariance matrix, GPJax uses <code class="docutils literal notranslate"><span class="pre">tfb.FillScaleTriL</span></code> transformation by default. <code class="docutils literal notranslate"><span class="pre">tfb.FillScaleTriL</span></code> fills a 1d vector into a lower triangular matrix and then applies <code class="docutils literal notranslate"><span class="pre">Softplus</span></code> transformation on the diagonal to satisfy the necessary conditions for a valid Cholesky matrix. Users can change this default transformation with another valid transformation of their choice. For example, <code class="docutils literal notranslate"><span class="pre">Square</span></code> transformation on the diagonal can also serve the purpose.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gpx_config</span> <span class="o">=</span> <span class="n">get_global_config</span><span class="p">()</span>
<span class="n">transformations</span> <span class="o">=</span> <span class="n">gpx_config</span><span class="o">.</span><span class="n">transformations</span>
<span class="n">jitter</span> <span class="o">=</span> <span class="n">gpx_config</span><span class="o">.</span><span class="n">jitter</span>

<span class="n">triangular_transform</span> <span class="o">=</span> <span class="n">dx</span><span class="o">.</span><span class="n">Chain</span><span class="p">(</span>
    <span class="p">[</span><span class="n">tfb</span><span class="o">.</span><span class="n">FillScaleTriL</span><span class="p">(</span><span class="n">diag_bijector</span><span class="o">=</span><span class="n">tfb</span><span class="o">.</span><span class="n">Square</span><span class="p">(),</span> <span class="n">diag_shift</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">jitter</span><span class="p">))]</span>
<span class="p">)</span>

<span class="n">transformations</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="s2">&quot;triangular_transform&quot;</span><span class="p">:</span> <span class="n">triangular_transform</span><span class="p">})</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/tmp/ipykernel_3143/780363273.py:1: DeprecatedWarning: get_global_config is deprecated as of 0.5.6 and will be removed in 0.6.0. Use method from jaxutils.config instead.
  gpx_config = get_global_config()
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">parameter_state</span> <span class="o">=</span> <span class="n">gpx</span><span class="o">.</span><span class="n">initialise</span><span class="p">(</span><span class="n">svgp</span><span class="p">,</span> <span class="n">key</span><span class="p">)</span>
<span class="n">optimiser</span> <span class="o">=</span> <span class="n">ox</span><span class="o">.</span><span class="n">adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>

<span class="n">inference_state</span> <span class="o">=</span> <span class="n">gpx</span><span class="o">.</span><span class="n">fit_batches</span><span class="p">(</span>
    <span class="n">objective</span><span class="o">=</span><span class="n">negative_elbo</span><span class="p">,</span>
    <span class="n">parameter_state</span><span class="o">=</span><span class="n">parameter_state</span><span class="p">,</span>
    <span class="n">train_data</span><span class="o">=</span><span class="n">D</span><span class="p">,</span>
    <span class="n">optax_optim</span><span class="o">=</span><span class="n">optimiser</span><span class="p">,</span>
    <span class="n">num_iters</span><span class="o">=</span><span class="mi">3000</span><span class="p">,</span>
    <span class="n">key</span><span class="o">=</span><span class="n">jr</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">42</span><span class="p">),</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">learned_params</span><span class="p">,</span> <span class="n">training_history</span> <span class="o">=</span> <span class="n">inference_state</span><span class="o">.</span><span class="n">unpack</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<script type="application/vnd.jupyter.widget-view+json">
{"version_major": 2, "version_minor": 0, "model_id": "054787425a97468993bf84aea04c3067"}
</script></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">latent_dist</span> <span class="o">=</span> <span class="n">q</span><span class="p">(</span><span class="n">learned_params</span><span class="p">)(</span><span class="n">xtest</span><span class="p">)</span>
<span class="n">predictive_dist</span> <span class="o">=</span> <span class="n">likelihood</span><span class="p">(</span><span class="n">learned_params</span><span class="p">,</span> <span class="n">latent_dist</span><span class="p">)</span>

<span class="n">meanf</span> <span class="o">=</span> <span class="n">predictive_dist</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="n">predictive_dist</span><span class="o">.</span><span class="n">stddev</span><span class="p">()</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s2">&quot;o&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.15</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Training Data&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:gray&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xtest</span><span class="p">,</span> <span class="n">meanf</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Posterior mean&quot;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;tab:blue&quot;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">xtest</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">meanf</span> <span class="o">-</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">meanf</span> <span class="o">+</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">)</span>
<span class="p">[</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">z_i</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;black&quot;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">z_i</span> <span class="ow">in</span> <span class="n">learned_params</span><span class="p">[</span><span class="s2">&quot;variational_family&quot;</span><span class="p">][</span><span class="s2">&quot;inducing_inputs&quot;</span><span class="p">]</span>
<span class="p">]</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/uncollapsed_vi_21_0.png" src="../_images/uncollapsed_vi_21_0.png" />
</div>
</div>
<p>We can see that <code class="docutils literal notranslate"><span class="pre">Square</span></code> transformation is able to get relatively better fit compared to <code class="docutils literal notranslate"><span class="pre">Softplus</span></code> with the same number of iterations, but <code class="docutils literal notranslate"><span class="pre">Softplus</span></code> is recommended over <code class="docutils literal notranslate"><span class="pre">Square</span></code> for stability of optimisation.</p>
</section>
<section id="system-configuration">
<h2>System configuration<a class="headerlink" href="#system-configuration" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%</span><span class="k">reload_ext</span> watermark
<span class="o">%</span><span class="k">watermark</span> -n -u -v -iv -w -a &#39;Thomas Pinder, Daniel Dodd &amp; Zeel B Patel&#39;
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Author: Thomas Pinder, Daniel Dodd &amp; Zeel B Patel

Last updated: Sat Jan 14 2023

Python implementation: CPython
Python version       : 3.9.16
IPython version      : 8.8.0

tensorflow_probability: 0.19.0
matplotlib            : 3.3.3
distrax               : 0.1.2
jaxkern               : 0.0.4
gpjax                 : 0.5.8
jax                   : 0.4.1
optax                 : 0.1.4

Watermark: 2.3.1
</pre></div>
</div>
</div>
</div>
</section>
</section>


<script type="application/vnd.jupyter.widget-state+json">
{"state": {"ba5db0f95fd842a69596716a251b2dd8": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "325c1a911382427fb477df0a3f24d218": {"model_name": "ProgressStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "ffef70f08a064ab6b25eaf4f65d400d8": {"model_name": "FloatProgressModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_ba5db0f95fd842a69596716a251b2dd8", "max": 3000.0, "min": 0.0, "orientation": "horizontal", "style": "IPY_MODEL_325c1a911382427fb477df0a3f24d218", "value": 3000.0}}, "649f71886de246b0aabea7f52bb92593": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "e101307208cd4d25b39240ea576a6f4b": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "5245c9deda5a4cf796f83f87391df2f9": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_649f71886de246b0aabea7f52bb92593", "placeholder": "\u200b", "style": "IPY_MODEL_e101307208cd4d25b39240ea576a6f4b", "value": "100%"}}, "f646cf53edea4472b76e765cb4f50058": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "47b14b5767e141e6b3b173ea5b5773a3": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "f3a185e4cac045bba52d0c3e29cc5cda": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_f646cf53edea4472b76e765cb4f50058", "placeholder": "\u200b", "style": "IPY_MODEL_47b14b5767e141e6b3b173ea5b5773a3", "value": " 3000/3000 [00:04&lt;00:00, 728.08it/s, Objective=7162.78]"}}, "bbfb2ab905514e668b3b9dd7f3ef462d": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "931074abc78642488b89d0f79068a02c": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_5245c9deda5a4cf796f83f87391df2f9", "IPY_MODEL_ffef70f08a064ab6b25eaf4f65d400d8", "IPY_MODEL_f3a185e4cac045bba52d0c3e29cc5cda"], "layout": "IPY_MODEL_bbfb2ab905514e668b3b9dd7f3ef462d"}}, "0be01650b5e84293abf1db5a2baf71ae": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "01100e65ae0149378d991d085acee66a": {"model_name": "ProgressStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "ProgressStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "bar_color": null, "description_width": ""}}, "2f44c52dccfd446caece45590db27a0e": {"model_name": "FloatProgressModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "FloatProgressModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "ProgressView", "bar_style": "success", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_0be01650b5e84293abf1db5a2baf71ae", "max": 3000.0, "min": 0.0, "orientation": "horizontal", "style": "IPY_MODEL_01100e65ae0149378d991d085acee66a", "value": 3000.0}}, "68043647382946afbf117602ee28aa51": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "fe5efd9a80654814970e96470b24ce2d": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "038daeb69d9546bb8f6f0f4d0eb4362d": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_68043647382946afbf117602ee28aa51", "placeholder": "\u200b", "style": "IPY_MODEL_fe5efd9a80654814970e96470b24ce2d", "value": "100%"}}, "01641a24a0574e3b88684c0829b621ec": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "9e2dadf4214341fc9f2baf33aaf4d481": {"model_name": "DescriptionStyleModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "DescriptionStyleModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "StyleView", "description_width": ""}}, "eb81f5803c204a30b7b78d26c216141c": {"model_name": "HTMLModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HTMLModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HTMLView", "description": "", "description_tooltip": null, "layout": "IPY_MODEL_01641a24a0574e3b88684c0829b621ec", "placeholder": "\u200b", "style": "IPY_MODEL_9e2dadf4214341fc9f2baf33aaf4d481", "value": " 3000/3000 [00:04&lt;00:00, 734.76it/s, Objective=2228.66]"}}, "9b00063c06654b40849e2ffab677b070": {"model_name": "LayoutModel", "model_module": "@jupyter-widgets/base", "model_module_version": "1.2.0", "state": {"_model_module": "@jupyter-widgets/base", "_model_module_version": "1.2.0", "_model_name": "LayoutModel", "_view_count": null, "_view_module": "@jupyter-widgets/base", "_view_module_version": "1.2.0", "_view_name": "LayoutView", "align_content": null, "align_items": null, "align_self": null, "border": null, "bottom": null, "display": null, "flex": null, "flex_flow": null, "grid_area": null, "grid_auto_columns": null, "grid_auto_flow": null, "grid_auto_rows": null, "grid_column": null, "grid_gap": null, "grid_row": null, "grid_template_areas": null, "grid_template_columns": null, "grid_template_rows": null, "height": null, "justify_content": null, "justify_items": null, "left": null, "margin": null, "max_height": null, "max_width": null, "min_height": null, "min_width": null, "object_fit": null, "object_position": null, "order": null, "overflow": null, "overflow_x": null, "overflow_y": null, "padding": null, "right": null, "top": null, "visibility": null, "width": null}}, "054787425a97468993bf84aea04c3067": {"model_name": "HBoxModel", "model_module": "@jupyter-widgets/controls", "model_module_version": "1.5.0", "state": {"_dom_classes": [], "_model_module": "@jupyter-widgets/controls", "_model_module_version": "1.5.0", "_model_name": "HBoxModel", "_view_count": null, "_view_module": "@jupyter-widgets/controls", "_view_module_version": "1.5.0", "_view_name": "HBoxView", "box_style": "", "children": ["IPY_MODEL_038daeb69d9546bb8f6f0f4d0eb4362d", "IPY_MODEL_2f44c52dccfd446caece45590db27a0e", "IPY_MODEL_eb81f5803c204a30b7b78d26c216141c"], "layout": "IPY_MODEL_9b00063c06654b40849e2ffab677b070"}}}, "version_major": 2, "version_minor": 0}
</script>


    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./nbs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="collapsed_vi.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Sparse Gaussian Process Regression</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="barycentres.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Gaussian Processes Barycentres</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The Jupyter Book Community<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>